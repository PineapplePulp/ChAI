{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 3\n",
    "images = torch.zeros(num_images,512,512)\n",
    "for i in range(num_images):\n",
    "    images[i,:,:] = torch.randn(512,512)\n",
    "\n",
    "images = images.reshape(512,512,num_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat.device = cpu\n",
      "cat.shape = torch.Size([512, 512])\n"
     ]
    }
   ],
   "source": [
    "cat = images[:,:,0]\n",
    "print('cat.device =',cat.device)\n",
    "print('cat.shape =', cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat.shape = torch.Size([512, 512, 1])\n"
     ]
    }
   ],
   "source": [
    "cat3 = torch.unsqueeze(cat,2)\n",
    "print('cat.shape =', cat3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3,4,5)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "permute() missing 1 required positional arguments: \"dims\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: permute() missing 1 required positional arguments: \"dims\""
     ]
    }
   ],
   "source": [
    "y = torch.permute(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2],\n",
       "         [3, 4, 5]]),\n",
       " torch.Size([2, 3]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(6).reshape(2, 3)\n",
    "t,t.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 3],\n",
       "        [1, 4],\n",
       "        [2, 5]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.permute(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2],\n",
       "         [3, 4, 5]]),\n",
       " torch.Size([2, 3]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t,t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0],\n",
       "         [1],\n",
       "         [2]],\n",
       "\n",
       "        [[3],\n",
       "         [4],\n",
       "         [5]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = t.reshape(2,3,1)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0, 0],\n",
       "         [1, 1, 1, 1],\n",
       "         [2, 2, 2, 2]],\n",
       "\n",
       "        [[3, 3, 3, 3],\n",
       "         [4, 4, 4, 4],\n",
       "         [5, 5, 5, 5]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.expand(2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(15).reshape(3,5)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 18, 21, 24, 27])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 10])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tensor.dim>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
       "        [ 5.,  6.,  7.,  8.,  9.],\n",
       "        [10., 11., 12., 13., 14.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(15,dtype=torch.float).reshape(3,5)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15., 18., 21., 24., 27.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 18., 21., 24., 27.],\n",
       "        [15., 18., 21., 24., 27.],\n",
       "        [15., 18., 21., 24., 27.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sum(dim=0).expand(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/r6tfrh3s0y16r9dl8fzkgdgw0000gp/T/ipykernel_9514/3741574789.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(torch.arange(15,dtype=torch.float).reshape(3,5),requires_grad=True)\n",
      "/var/folders/8v/r6tfrh3s0y16r9dl8fzkgdgw0000gp/T/ipykernel_9514/3741574789.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  u = torch.tensor(torch.arange(15,dtype=torch.float).reshape(3,5),requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
       "         [ 5.,  6.,  7.,  8.,  9.],\n",
       "         [10., 11., 12., 13., 14.]], requires_grad=True),\n",
       " None)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor(torch.arange(15,dtype=torch.float).reshape(3,5),requires_grad=True)\n",
    "u = torch.tensor(torch.arange(15,dtype=torch.float).reshape(3,5),requires_grad=True)\n",
    "\n",
    "t,t.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   1.,   4.,   9.,  16.],\n",
       "        [ 25.,  36.,  49.,  64.,  81.],\n",
       "        [100., 121., 144., 169., 196.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = t * u\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/r6tfrh3s0y16r9dl8fzkgdgw0000gp/T/ipykernel_9514/2427823174.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  s,s.grad\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(1015., grad_fn=<SumBackward1>), None)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = v.sum(dim=(0,1))\n",
    "s,s.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/r6tfrh3s0y16r9dl8fzkgdgw0000gp/T/ipykernel_9514/2815799162.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  s.grad\n"
     ]
    }
   ],
   "source": [
    "s.backward()\n",
    "s.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   1.,   4.,   9.,  16.],\n",
       "        [ 25.,  36.,  49.,  64.,  81.],\n",
       "        [100., 121., 144., 169., 196.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/r6tfrh3s0y16r9dl8fzkgdgw0000gp/T/ipykernel_9514/337251670.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  t.grad,u.grad,v.grad\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
       "         [ 5.,  6.,  7.,  8.,  9.],\n",
       "         [10., 11., 12., 13., 14.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
       "         [ 5.,  6.,  7.,  8.,  9.],\n",
       "         [10., 11., 12., 13., 14.]]),\n",
       " None)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.grad,u.grad,v.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/r6tfrh3s0y16r9dl8fzkgdgw0000gp/T/ipykernel_9514/3640305849.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  v.grad\n"
     ]
    }
   ],
   "source": [
    "v.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.ones(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[4]' is invalid for input of size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[4]' is invalid for input of size 1"
     ]
    }
   ],
   "source": [
    "z.reshape(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/r6tfrh3s0y16r9dl8fzkgdgw0000gp/T/ipykernel_9514/1768566735.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  M = torch.tensor(torch.arange(15).reshape(5,3),requires_grad=True,dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 5., 14., 23., 32., 41.], grad_fn=<MvBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = torch.tensor(torch.arange(15).reshape(5,3),requires_grad=True,dtype=torch.float32)\n",
    "v = torch.arange(3,requires_grad=True,dtype=torch.float32)\n",
    "y = torch.matmul(M,v)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.],\n",
       "        [ 3.,  4.,  5.],\n",
       "        [ 6.,  7.,  8.],\n",
       "        [ 9., 10., 11.],\n",
       "        [12., 13., 14.]], requires_grad=True)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = y.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [0., 1., 2.],\n",
       "        [0., 1., 2.],\n",
       "        [0., 1., 2.],\n",
       "        [0., 1., 2.]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/r6tfrh3s0y16r9dl8fzkgdgw0000gp/T/ipykernel_9514/3613637652.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  M = torch.tensor(torch.arange(15).reshape(5,3),requires_grad=True,dtype=torch.float32)\n",
      "/var/folders/8v/r6tfrh3s0y16r9dl8fzkgdgw0000gp/T/ipykernel_9514/3613637652.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  v = torch.tensor(torch.arange(9).reshape(3,3),requires_grad=True,dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "M = torch.tensor(torch.arange(15).reshape(5,3),requires_grad=True,dtype=torch.float32)\n",
    "v = torch.tensor(torch.arange(9).reshape(3,3),requires_grad=True,dtype=torch.float32)\n",
    "y = torch.matmul(M,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 15.,  18.,  21.],\n",
       "         [ 42.,  54.,  66.],\n",
       "         [ 69.,  90., 111.],\n",
       "         [ 96., 126., 156.],\n",
       "         [123., 162., 201.]], grad_fn=<MmBackward0>),\n",
       " torch.Size([5, 3]))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3., grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 4., 5.],\n",
       "        [6., 7., 8.]], requires_grad=True)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.],\n",
       "        [ 3.,  4.,  5.],\n",
       "        [ 6.,  7.,  8.],\n",
       "        [ 9., 10., 11.],\n",
       "        [12., 13., 14.]], requires_grad=True)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  5.,  14.,  23.,  32.,  41.],\n",
       "        [ 14.,  50.,  86., 122., 158.],\n",
       "        [ 23.,  86., 149., 212., 275.]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ = torch.einsum('ij,bj->bi',M,v)\n",
    "y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_.sum(0).sum(0).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9., 12., 15.],\n",
       "        [ 9., 12., 15.],\n",
       "        [ 9., 12., 15.],\n",
       "        [ 9., 12., 15.],\n",
       "        [ 9., 12., 15.]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(8).reshape(2, 1, 2, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 2])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.squeeze(x,(1,2,3)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5],\n",
       "        [6, 7],\n",
       "        [8, 9]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(10).reshape(5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 528,  564,  600,  636,  672,  708,  744],\n",
       "          [ 852,  888,  924,  960,  996, 1032, 1068],\n",
       "          [1176, 1212, 1248, 1284, 1320, 1356, 1392],\n",
       "          [1500, 1536, 1572, 1608, 1644, 1680, 1716],\n",
       "          [1824, 1860, 1896, 1932, 1968, 2004, 2040],\n",
       "          [2148, 2184, 2220, 2256, 2292, 2328, 2364],\n",
       "          [2472, 2508, 2544, 2580, 2616, 2652, 2688]]]),\n",
       " torch.Size([1, 7, 7]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.arange(1 * 9 * 9).reshape(1,9,9)\n",
    "ker = torch.arange(1 * 3 * 3).reshape(1,1,3,3)\n",
    "fet = torch.conv2d(img,ker,stride=1,dilation=1)\n",
    "fet,fet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/r6tfrh3s0y16r9dl8fzkgdgw0000gp/T/ipykernel_9514/3917236685.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(torch.arange(3 * 9 * 9).reshape(3,9,9),requires_grad=True,dtype=torch.float32)\n",
      "/var/folders/8v/r6tfrh3s0y16r9dl8fzkgdgw0000gp/T/ipykernel_9514/3917236685.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ker = torch.tensor(torch.arange(3 * 3 * 3).reshape(1,3,3,3),requires_grad=True,dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(897552., grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.tensor(torch.arange(3 * 9 * 9).reshape(3,9,9),requires_grad=True,dtype=torch.float32)\n",
    "ker = torch.tensor(torch.arange(3 * 3 * 3).reshape(1,3,3,3),requires_grad=True,dtype=torch.float32)\n",
    "fet = torch.conv2d(img,ker,stride=2,dilation=1)\n",
    "x = fet.sum(0).sum(0).sum(0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/r6tfrh3s0y16r9dl8fzkgdgw0000gp/T/ipykernel_9514/2392927721.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(torch.arange(1 * 9 * 9).reshape(1,9,9),requires_grad=True,dtype=torch.float32)\n",
      "/var/folders/8v/r6tfrh3s0y16r9dl8fzkgdgw0000gp/T/ipykernel_9514/2392927721.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ker = torch.tensor(torch.arange(1 * 3 * 3).reshape(1,1,3,3),requires_grad=True,dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.,  1.,  3.,  3.,  3.,  3.,  3.,  3.,  2.],\n",
       "          [ 3.,  8., 15., 15., 15., 15., 15., 12.,  7.],\n",
       "          [ 9., 21., 36., 36., 36., 36., 36., 27., 15.],\n",
       "          [ 9., 21., 36., 36., 36., 36., 36., 27., 15.],\n",
       "          [ 9., 21., 36., 36., 36., 36., 36., 27., 15.],\n",
       "          [ 9., 21., 36., 36., 36., 36., 36., 27., 15.],\n",
       "          [ 9., 21., 36., 36., 36., 36., 36., 27., 15.],\n",
       "          [ 9., 20., 33., 33., 33., 33., 33., 24., 13.],\n",
       "          [ 6., 13., 21., 21., 21., 21., 21., 15.,  8.]]]),\n",
       " torch.Size([1, 9, 9]),\n",
       " tensor([[[[1470., 1519., 1568.],\n",
       "           [1911., 1960., 2009.],\n",
       "           [2352., 2401., 2450.]]]]))"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "img = torch.tensor(torch.arange(1 * 9 * 9).reshape(1,9,9),requires_grad=True,dtype=torch.float32)\n",
    "ker = torch.tensor(torch.arange(1 * 3 * 3).reshape(1,1,3,3),requires_grad=True,dtype=torch.float32)\n",
    "fet = torch.conv2d(img,ker,stride=1,dilation=1)\n",
    "x = fet.sum(0).sum(0).sum(0)\n",
    "x.backward()\n",
    "img.grad,img.grad.shape,ker.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 7])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Calculated padded input size per channel: (7 x 7). Kernel size: (9 x 9). Kernel size can't be greater than actual input size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[188], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (7 x 7). Kernel size: (9 x 9). Kernel size can't be greater than actual input size"
     ]
    }
   ],
   "source": [
    "torch.conv2d(torch.zeros(1,7,7),torch.zeros(1,1,9,9),padding=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],\n",
       "          [ 9., 10., 11., 12., 13., 14., 15., 16., 17.],\n",
       "          [18., 19., 20., 21., 22., 23., 24., 25., 26.],\n",
       "          [27., 28., 29., 30., 31., 32., 33., 34., 35.],\n",
       "          [36., 37., 38., 39., 40., 41., 42., 43., 44.],\n",
       "          [45., 46., 47., 48., 49., 50., 51., 52., 53.],\n",
       "          [54., 55., 56., 57., 58., 59., 60., 61., 62.],\n",
       "          [63., 64., 65., 66., 67., 68., 69., 70., 71.],\n",
       "          [72., 73., 74., 75., 76., 77., 78., 79., 80.]]], requires_grad=True),\n",
       " tensor([[[ 528.,  564.,  600.,  636.,  672.,  708.,  744.],\n",
       "          [ 852.,  888.,  924.,  960.,  996., 1032., 1068.],\n",
       "          [1176., 1212., 1248., 1284., 1320., 1356., 1392.],\n",
       "          [1500., 1536., 1572., 1608., 1644., 1680., 1716.],\n",
       "          [1824., 1860., 1896., 1932., 1968., 2004., 2040.],\n",
       "          [2148., 2184., 2220., 2256., 2292., 2328., 2364.],\n",
       "          [2472., 2508., 2544., 2580., 2616., 2652., 2688.]]],\n",
       "        grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img,fet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/r6tfrh3s0y16r9dl8fzkgdgw0000gp/T/ipykernel_57611/3913852475.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(torch.arange(1 * 9 * 9).reshape(1,9,9),requires_grad=True,dtype=torch.float32)\n",
      "/var/folders/8v/r6tfrh3s0y16r9dl8fzkgdgw0000gp/T/ipykernel_57611/3913852475.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ker = torch.tensor(torch.arange(1 * 3 * 3).reshape(1,1,3,3),requires_grad=True,dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 528.,  600.,  672.,  744.],\n",
       "         [1176., 1248., 1320., 1392.],\n",
       "         [1824., 1896., 1968., 2040.],\n",
       "         [2472., 2544., 2616., 2688.]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.tensor(torch.arange(1 * 9 * 9).reshape(1,9,9),requires_grad=True,dtype=torch.float32)\n",
    "ker = torch.tensor(torch.arange(1 * 3 * 3).reshape(1,1,3,3),requires_grad=True,dtype=torch.float32)\n",
    "fet = torch.conv2d(img,ker,stride=2,dilation=1)\n",
    "x = fet.sum(0).sum(0).sum(0)\n",
    "x.backward()\n",
    "img.grad,img.grad.shape,ker.grad\n",
    "fet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
